---
<br>title: 梯度消失和梯度爆炸
<br>date: '2019-04-21 13:53:51'
<br>updated: '2019-04-21 14:10:24'
<br>tags:
<br>  - 深度学习
<br>  - 激活函数
<br>  - 梯度消失
<br>  - 梯度爆炸
<br>categories:
<br>  - 深度学习
<br>abbrlink: 326a3382
<br>---
<br>梯度消失 和 梯度爆炸
<br>-----------
<br>
<br><h4>1 是什么</h4><br>
<br>首先，BP神经网络基于梯度下降策略，以目标的负梯度方向进行权重更新，$\omega \leftarrow \omega + \Delta\omega$, 给定学习率$\alpha, \Delta \omega = -\alpha \times \frac {\partial{Loss}}{\partial \omega}$。假设每层全连接网络激活函数为$f(\dot )$, 则$i+1$层的输入$f_{i+1}= f(f_i * w_{i+1}+ b_{i+1})$ , 则$\frac{\partial{f_{i+1}}}{\partial{w_{i+1}}}= f_i$
<br>
<br>根据链式求导法则，当需要更新第二层隐层梯度信息时: $\Delta w_{1}=\frac{\partial \text {Loss}}{\partial w_{2}}=\frac{\partial {Los}s}{\partial f_{4}} \frac{\partial f_{4}}{\partial f_{3}} \frac{\partial f_{3}}{\partial f_{2}} \frac{\partial f_{2}}{\partial w_{2}}$，又 $ \frac{\partial f_{2}}{\partial w_{2}}=f_1$。发现每一层的更新都需要求激活函数在上层输出值下的导数值。如果激活函数>1或<1，在层数加深时，导数就会呈指数型变化，就可能产生梯度消失或梯度爆炸。
<br>
<br>接下来，我们来看一段代码。
<br>
<br>```python
<br> class TorchNet():
<br>    dtype = torch.float
<br>    device = torch.device('cpu')
<br>	
<br>    # 定义batch size, 输入特征数， 隐层特征，输出数
<br>    N, D_in, H, D_out = 64, 1000, 100, 10
<br>
<br>    # 随机初始化
<br>    x = torch.randn(N, D_in, device=device, dtype=dtype)
<br>    y = torch.randn(N, D_out, device=device, dtype=dtype)
<br>
<br>    w1 = torch.randn(D_in, H, device=device, dtype=dtype)
<br>    w2 = torch.randn(H, D_out, device=device, dtype=dtype)
<br>	# 定义学习率，可以修改一下，看看结果
<br>    learning_rate = 1e-6
<br>    for t in range(500):
<br>        # forward
<br>        h = x.mm(w1)
<br>        h_relu = h.clamp(min=0)
<br>        y_pred = h_relu.mm(w2)
<br>
<br>        # compute loss
<br>        loss = (y_pred - y).pow(2).sum().item()
<br>        print(t, loss)
<br>
<br>        # backprop to compute gradients
<br>        grad_y_pred = 2 * (y_pred - y)
<br>        grad_w2 = h_relu.t().mm(grad_y_pred)
<br>        grad_h_relu = grad_y_pred.mm(w2.t())
<br>        grad_h = grad_h_relu.clone()
<br>        grad_h[h<0] = 0
<br>        grad_w1 = x.t().mm(grad_h)
<br>
<br>        # update gradientts
<br>        w1 -= learning_rate * grad_w1
<br>        w2 -= learning_rate * grad_w2
<br>```
<br>
<br>（可以通过链式求导法则理解）我们看到，权值`w`的更新和梯度息息相关，每次反向传播 ​, 如果激活函数的导数趋近于0，那么权值在多重传播之后可能不再更新，则是梯度消失；如果梯度取值大于1，经过多层传播之后，权值的调整就会变得很大，导致网络不稳定。
<br>
<br>这些问题都是“反向传播训练法则”所具有的先天问题。
<br>
<br><h4>2. 如何判断</h4><br>
<br>1.  看loss的变化/权值/参数的变化是否稳定，或者无法更新
<br>    
<br>2.  loss是否变成了NaN
<br>    
<br>3.  权重是否变成NaN
<br>    
<br>
<br><h4>3. 如何解决出现的问题；如何避免</h4><br>
<br>我们看到，梯度爆炸或者消失的根本原因来自于激活函数，同时，激活函数的导数值又影响实际的梯度更新，因此我们考虑从激活函数和函数的导数值来解决激活函数的问题。
<br>
<br><h4>3.1 重新设计网络</h4><br>
<br>1.  层数更少的简单网络能够降低梯度消失和梯度爆炸的影响
<br>    
<br>2.  更小的训练批次也能在实验中起效果
<br>    
<br>3.  截断传播的层数
<br>    
<br>4.  同样，长短期记忆网络（LSTM）和相关的门单元也能减少梯度爆炸。
<br>    
<br>5. 对网络权重使用正则，防止过拟合。$$Loss =(y-W\^Tx)\^{2}+\alpha\|W\|^2$$
<br>
<br>    
<br>
<br><h4>3.2 修改激活函数</h4><br>
<br>1.  修改为ReLU，leaky ReLU，PReLU，ELU，SELU，都可以。也可以使用maxout
<br>    
<br>
<br><h4>3.3 使用梯度截断（Gradient clipping）</h4><br>
<br>对大型深层网络，还是要检查和限制梯度大小，进行梯度截断。WGAN中，限制梯度更新是为了保证lipchitz条件。
<br>
<br><h4>3.4 Batch Normalization</h4><br>
<br>**Batchnorm** 具有加速网络收敛速度，提升训练稳定性的效果，通过规范化操作将输出信号x规范化到均值为0、方差为1，保证网络的稳定性。batchnorm在反向传播的过程中，由于对输入信号做了scale和shift，通过观察[激活函数及其函数的图像](https://www.jithub.cn/articles/2019/04/18/1555558277373.html#b3_solo_h2_2)(请点击链接中的超链)，这样可以是激活函数的值落在对非线性函数比较敏感的区域。这样也会使损失函数产生较大的变化，让梯度整体变大，避免梯度消失；同时也意味着收敛速度更快，学习速度更快。
<br>
<br><h4>3.5 ResNet，残差网络结构</h4><br>
<br>终结者，ResNet。
<br>
<br>