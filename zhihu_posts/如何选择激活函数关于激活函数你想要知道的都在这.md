---
<br>title: 如何选择激活函数；关于激活函数，你想要知道的都在这
<br>date: '2019-04-18 11:31:17'
<br>updated: '2019-04-20 19:40:20'
<br>tags:
<br>  - deeplearning
<br>  - 激活函数
<br>  - relu
<br>  - 可视化
<br>mathjax: true
<br>categories:
<br>  - 深度学习
<br>abbrlink: 567b1a85
<br>---
<br><h3>激活函数</h3><br>对于神经网络，网络的每一层计算可以理解为$f(wx+b)=f(w\'x)$，每一层可以认为是每个权重矩阵$W$ 乘输入特征矩阵$X$。根据矩阵乘法，如果是线性激励，即激活函数为$f(x) = x$，多层网络相当于一层网络。比如：$​​f(W_1*f(W_2x))=W_1 W_2x=Wx$ 。
<br>因此，需要引入非线性激活函数，对特征进行充分组合。也可以说相当于进行空间变换，使得原来线性不可解的问题有非线性的表达，找到非线性的解。
<br><h3>好的激活函数的特性</h3><br>1. 非线性，保证不会退回线性网络。
<br>2. 几乎处处可微：可微性保证了在优化中梯度的可计算性。（有限不可微的点影响不大），即保证了直接利用数值优化的方法学习网络参数的可能
<br>3. 计算简单/计算量小：提高网络计算效率。
<br>4. 非饱和性（no-saturation): 数学概念中，饱和指的是在某些区间梯度接近于零（即梯度消失），若$x \rightarrow-\infty$时导数$f^{\prime}(x) \rightarrow 0$，则称为左饱和；另一端为右饱和。使得参数无法继续更新的问题。ReLU在x<0的情况下，  梯度为0，也饱和，即dying  ReLU
<br>5.  单调性：导数符号不变
<br>6.  输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定（但可能会导致梯度消失，或者限制神经元表达能力）
<br>7.  接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。
<br>8.  参数少：Maxout，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。
<br>9.  归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch  Normalization。
<br><h4>如何选择激活函数</h4><br>1. 首先尝试ReLU,速度快,但要注意训练的状态.
<br>2. 如果ReLU效果欠佳,尝试Leaky ReLU或Maxout等变种。
<br>3. 尝试tanh正切函数(以零点为中心,零点处梯度为1)
<br>4. sigmoid型函数（logistic/tanh）在RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值.
<br>5. 在浅层神经网络中，如不超过4层的，可选择使用多种激励函数，没有太大的影响.
<br>6. Kaiming He 也验证过maxout几乎能到the state of art的结果。两个maxout函数几乎可以拟合所有凸函数。
<br><h4>特例：LSTM</h4><br>sigmoid/tanh在RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。对于用在了状态和输出上，是对数据的处理，使用何种激活函数参考上述内容。
<br><h4>梯度消失问题和关于原点对称/以0为中心的问题</h4><br>Sigmoid 型函数是指一类 S 型曲线函数，为两端饱和函数。常用的 Sigmoid型函数有 Logistic 函数和 Tanh 函数。
<br>Tanh函数相当于放大并平移的logistic函数，$\tanh (x)=2 \sigma(2 x)-1$。因为这两个函数都是在中间(0 附近)近似线性，两端饱和。因此，这两个函数可以通过分段函数来近似。通过x=0位置的一阶泰勒展开可以得到分段函数近似的hard-sigmoid函数。
<br>1. 解释tanh 为什么相对收敛更快。（主要是tanh梯度消失问题较轻）
<br>   $\begin{array}{l}{\tanh ^{\prime}(x)=1-\tanh (x)^{2} \in(0,1)} \\ {\text { sigmoid: } s^{\prime}(x)=s(x)\times(1-s(x)) \in(0,1 / 4)}\end{array}$
<br>2. 以零为中心的影响
<br>   Tanh函数的输出是零中心 化的(Zero-Centered)，而 Logistic 函数的输出恒大于 0。非零中心化的输出会 使得其后一层的神经元的输入发生偏置偏移(Bias Shift)，并进一步使得梯度下降的收敛速度变慢。
<br>   如果当前参数$(w_0,w_1)$的最佳优化方向是$(+d_0, -d_1)$,则根据反向传播计算公式,我们希望$x_0$ 和 $x_1$符号相反。但是如果上一级神经元采用 Sigmoid 函数作为激活函数，sigmoid不以0为中心，输出值恒为正，那么我们无法进行最快的参数更新，而是走 Z 字形逼近最优解。
<br>我们来看看，哪些激活函数能满足这些特性。
<br><h3>可视化</h3><br>从下面的菜单中选择激活函数，以绘制它及其一阶导数。右侧的框中提供了与神经网络相关的一些属性。
<br>[可视化详细页面](https://www.jithub.cn/fileserver/activation_fun_d3.html)
<br>{% raw %}
<br><html>
<br>  <head>
<br>    <style>
<br>      #visualisation {
<br>            height: 500px;
<br>      }
<br>      .axis path,
<br>      .axis line {
<br>        fill: none;
<br>        stroke: #777;
<br>        shape-rendering: crispEdges;
<br>      }
<br>      .axis text {
<br>        font-family: "Arial";
<br>        font-size: 0.6em;
<br>      }
<br>      .tick {
<br>        stroke-dasharray: 1, 2;
<br>      }
<br>      .bar {
<br>        fill: FireBrick;
<br>      }
<br>      .legend {
<br>        cursor: pointer;
<br>      }
<br>      .MathJax_SVG svg > g,
<br>      .MathJax_SVG_Display svg > g {
<br>        fill: black;
<br>        stroke: black;
<br>      }
<br>      .legend {
<br>        font-size: 1.2em;
<br>      }
<br>      .graph_inputs {
<br>        font-size: 0.8em;
<br>      }
<br>      .func_text {
<br>        font-size: 0.65em;
<br>      }
<br>      .deriv_func_text {
<br>        font-size: 0.65em;
<br>      }
<br>      #select_activ {
<br>        font-size: 0.8em;
<br>      }
<br>      .input_labels {
<br>        font-size: 0.8em;
<br>      }
<br>      #func_equation {
<br>        font: "Helvetica Neue";
<br>        font-size: 0.9em;
<br>      }
<br>      #deriv_func_equation {
<br>        font: "Helvetica Neue";
<br>        font-size: 0.9em;
<br>      }
<br>      #rrelu_button {
<br>        background-color: #009900;
<br>        color: white;
<br>        text-align: center;
<br>        text-decoration: none;
<br>        display: inline-block;
<br>        font-size: 0.8em;
<br>        cursor: pointer;
<br>        margin-left: 0.3em;
<br>      }
<br>      #rrelu_button:hover {
<br>        background-color: #4caf50;
<br>        color: white;
<br>      }
<br>    </style>
<br>    <script type="text/javascript" src="https://d3js.org/d3.v3.min.js"></script>
<br>    <script
<br>      type="text/javascript"
<br>      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML"
<br>    ></script>
<br>  </head>
<br>  <body>
<br>    <div id="browser_warning">
<br>      <p>
<br>        版权所有为
<br>        <a href="https://dashee87.github.io/about/">David Sheehan</a>
<br>        原blog地址为：<a
<br>          href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/"
<br>          >Visualising Activation Functions in Neural Networks</a
<br>        >
<br>      </p>
<br>    </div>
<br>    <div id="browser_warning">
<br>      <p>
<br>        Note:
<br>        注意：建议您在Chrome上查看以获得最佳体验。在Firefox和IE上，框中的等式可能无法呈现。
<br>      </p>
<br>    </div>
<br>    <select id="select_activ" onchange="InitChart();">
<br>      <option value="step" selected="selected">Step</option>
<br>      <option value="identity">Identity</option>
<br>      <option value="relu">ReLu</option>
<br>      <option value="sigmoid">Sigmoid</option>
<br>      <option value="tanh">Tanh</option>
<br>      <option value="leakyrelu">Leaky ReLU</option>
<br>      <option value="prelu">PReLU</option>
<br>      <option value="rrelu">RReLU</option>
<br>      <option value="elu">ELU</option>
<br>      <option value="selu">SELU</option>
<br>      <option value="srelu">SReLU</option>
<br>      <!--<option value="probit">Probit</option>-->
<br>      <option value="hardsigmoid">Hard Sigmoid</option>
<br>      <option value="hardtanh">Hard Tanh</option>
<br>      <option value="lctanh">LeCun Tanh</option>
<br>      <option value="arctan">ArcTan</option>
<br>      <option value="softsign">SoftSign</option>
<br>      <option value="softplus">Softplus</option>
<br>      <option value="sign">Signum</option>
<br>      <option value="bentid">Bent Identity</option>
<br>      <option value="symmsigmoid">Symmetrical Sigmoid</option>
<br>      <option value="loglog">Log Log</option>
<br>      <option value="gaussian">Gaussian</option>
<br>      <option value="absolute">Absolute</option>
<br>      <option value="sinusoid">Sinusoid</option>
<br>      <option value="cos">Cos</option>
<br>      <option value="sinc">Sinc</option>
<br>    </select>
<br>    <form name="message" method="post" id="plot_inputs">
<br>      <div style="float:left;margin-right:20px;margin-left:100px;">
<br>        <label
<br>          for="alpha_input"
<br>          style="display: none"
<br>          class="input_labels"
<br>          id="alpha_input_label"
<br>          >α<sub> </sub
<br>        ></label>
<br>        <input
<br>          id="alpha_input"
<br>          class="graph_inputs"
<br>          size="5"
<br>          type="text"
<br>          value="0.7"
<br>          style="display: none;"
<br>          onchange="InitChart();"
<br>        />
<br>      </div>
<br>      <div style="float:left;margin-right:20px;">
<br>        <label
<br>          for="alpha_input_r"
<br>          style="display: none"
<br>          class="input_labels"
<br>          id="alpha_input_r_label"
<br>          >α<sub>r</sub></label
<br>        >
<br>        <input
<br>          id="alpha_input_r"
<br>          class="graph_inputs"
<br>          size="5"
<br>          type="text"
<br>          value="0.4"
<br>          style="display: none;"
<br>          onchange="InitChart();"
<br>        />
<br>      </div>
<br>      <div style="float:left;margin-right:20px;">
<br>        <label
<br>          for="t_input_l"
<br>          style="display: none"
<br>          class="input_labels"
<br>          id="t_input_l_label"
<br>          >t<sub>l</sub></label
<br>        >
<br>        <input
<br>          id="t_input_l"
<br>          class="graph_inputs"
<br>          size="5"
<br>          type="text"
<br>          value="-1.0"
<br>          style="display: none;"
<br>          onchange="InitChart();"
<br>        />
<br>      </div>
<br>      <div style="float:left;margin-right:20px;">
<br>        <label
<br>          for="t_input_r"
<br>          style="display: none"
<br>          class="input_labels"
<br>          id="t_input_r_label"
<br>          >t<sub>r</sub></label
<br>        >
<br>        <input
<br>          id="t_input_r"
<br>          class="graph_inputs"
<br>          size="5"
<br>          type="text"
<br>          value="1.0"
<br>          style="display: none;"
<br>          onchange="InitChart();"
<br>        />
<br>      </div>
<br>      <br style="clear:both;" />
<br>    </form>
<br>    <button
<br>      type="button"
<br>      onclick="InitChart();"
<br>      id="rrelu_button"
<br>      style="display: none;margin-left:20px;"
<br>    >
<br>      Random α
<br>    </button>
<br>    <div id="legendContainer" class="legendContainer" style="width:100%">
<br>      <svg id="visualisation" width="100%" height="500px" height="80vh"></svg>
<br>      <div
<br>        id="func_primer"
<br>        style="margin-left:20px;font-size:0.8em;margin-right:20px"
<br>      >
<br>        <p>
<br>          这种激活函数更具理论性而非实际性，模仿生物神经元的全有或全无特性。它对神经网络没有
<br>          用，因为它的导数是零（除了0，它是未定义的）。这意味着基于梯度的优化方法是不可行的。
<br>        </p>
<br>      </div>
<br>    </div>
<br>    <br />
<br>    <script type="text/javascript">
<br>      MathJax.Hub.Config({
<br>        tex2jax: {
<br>          inlineMath: [["$", "$"], ["\\(", "\\)"]],
<br>          processEscapes: true
<br>        }
<br>      });
<br>      function erf(x) {
<br>        var z;
<br>        const ERF_A = 0.147;
<br>        var the_sign_of_x;
<br>        if (0 == x) {
<br>          the_sign_of_x = 0;
<br>          return 0;
<br>        } else if (x > 0) {
<br>          the_sign_of_x = 1;
<br>        } else {
<br>          the_sign_of_x = -1;
<br>        }
<br>        var one_plus_axsqrd = 1 + ERF_A * x * x;
<br>        var four_ovr_pi_etc = 4 / Math.PI + ERF_A * x * x;
<br>        var ratio = four_ovr_pi_etc / one_plus_axsqrd;
<br>        ratio *= x * -x;
<br>        var expofun = Math.exp(ratio);
<br>        var radical = Math.sqrt(1 - expofun);
<br>        z = radical * the_sign_of_x;
<br>        return z;
<br>      }
<br>      // https://stackoverflow.com/questions/12556685/is-there-a-javascript-implementation-of-the-inverse-error-function-akin-to-matl
<br>      function erfINV(inputX) {
<br>        var _a = (8 * (Math.PI - 3)) / (3 * Math.PI * (4 - Math.PI));
<br>        var _x = parseFloat(inputX);
<br>        var signX = _x < 0 ? -1.0 : 1.0;
<br>        var oneMinusXsquared = 1.0 - _x * _x;
<br>        var LNof1minusXsqrd = Math.log(oneMinusXsquared);
<br>        var PI_times_a = Math.PI * _a;
<br>        var firstTerm = Math.pow(2.0 / PI_times_a + LNof1minusXsqrd / 2.0, 2);
<br>        var secondTerm = LNof1minusXsqrd / _a;
<br>        var thirdTerm = 2 / PI_times_a + LNof1minusXsqrd / 2.0;
<br>        var primaryComp = Math.sqrt(
<br>          Math.sqrt(firstTerm - secondTerm) - thirdTerm
<br>        );
<br>        var scaled_R = signX * primaryComp;
<br>        return scaled_R;
<br>      }
<br>      var delay = 200;
<br>      var activ_func = document.getElementById("select_activ").value;
<br>      var lineData = [];
<br>      var lineData_deriv = [];
<br>      func_tags = [
<br>        {
<br>          func_tex:
<br>            "$f(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\0 & \\text{for } x<0\\end{cases} $",
<br>          func_range: "{0, 1}",
<br>          func_contin: "C" + "⁻¹",
<br>          func_mono: "✅",
<br>          func_origin: "❌",
<br>          func_symm: "不对称",
<br>          func_ref: "",
<br>          func_primer:
<br>            "这种激活函数更具理论性而非实际性, 包括<b>Step</b> (也包括, <b>Binary Step</b> 或 <b>Heaviside</b>) 模仿生物神经元的全有或全无特性。它对神经网络没有用，因为它的导数是零（除了0，它是未定义的）。这意味着基于梯度的优化方法是不可行的"
<br>        }
<br>      ];
<br>      deriv_func_tags = [
<br>        {
<br>          deriv_func_tex:
<br>            "$f'(x) =\\begin{cases}0 & \\text{for } x\\neq0\\\\? & \\text{for } x=0\\end{cases} $",
<br>          deriv_func_range: "{0}",
<br>          deriv_func_contin: "❌",
<br>          deriv_func_mono: "❌",
<br>          deriv_func_symm: "❌",
<br>          deriv_func_van: "No",
<br>          deriv_func_exp: "No",
<br>          deriv_func_sat: "No",
<br>          deriv_func_dead: "Yes"
<br>        }
<br>      ];
<br>      axis_limits = [-3, 3, -1.3, 1.3];
<br>      for (var i = -3; i <= 3; i = i + 0.001) {
<br>        lineData.push({
<br>          x: i,
<br>          y: i >= 0
<br>        });
<br>        lineData_deriv.push({
<br>          x: i,
<br>          y: 0
<br>        });
<br>      }
<br>      var vis = d3.select("#visualisation"),
<br>        WIDTH = parseInt(
<br>          document.getElementById("browser_warning").offsetWidth
<br>        ),
<br>        HEIGHT = parseInt(vis.style("height")),
<br>        //   HEIGHT = parseInt(document.documentElement.clientHeight)*0.8,
<br>        MARGINS = {
<br>          top: HEIGHT / 25.0,
<br>          right: WIDTH / 2.5,
<br>          bottom: HEIGHT / 25.0,
<br>          left: WIDTH / 50.0
<br>        },
<br>        boxmargin = (HEIGHT - MARGINS.top) / 24.0,
<br>        boxmargin_horizontal = MARGINS.right / 2.2,
<br>        rect_margin = MARGINS.right / 50.0,
<br>        xRange = d3.scale
<br>          .linear()
<br>          .range([MARGINS.left, WIDTH - MARGINS.right])
<br>          .domain(axis_limits.slice(0, 2)),
<br>        yRange = d3.scale
<br>          .linear()
<br>          .range([HEIGHT - MARGINS.top, MARGINS.bottom])
<br>          .domain(axis_limits.slice(2, 4)),
<br>        xAxis = d3.svg
<br>          .axis()
<br>          .scale(xRange)
<br>          .tickSize(5)
<br>          .tickSubdivide(true),
<br>        yAxis = d3.svg
<br>          .axis()
<br>          .scale(yRange)
<br>          .tickSize(5)
<br>          .orient("left")
<br>          .tickSubdivide(true);
<br>      vis
<br>        .append("svg:g")
<br>        .attr("class", "x axis")
<br>        .attr(
<br>          "transform",
<br>          "translate(0," + (HEIGHT - MARGINS.bottom + MARGINS.top) / 2 + ")"
<br>        )
<br>        .attr("id", "graph_xaxis")
<br>        .call(xAxis);
<br>      vis
<br>        .append("svg:g")
<br>        .attr("class", "y axis")
<br>        .attr(
<br>          "transform",
<br>          "translate(" + (WIDTH - MARGINS.right + MARGINS.left) / 2 + ",0)"
<br>        )
<br>        .attr("id", "graph_yaxis")
<br>        .call(yAxis);
<br>      vis
<br>        .selectAll(".tick")
<br>        .filter(function(d) {
<br>          return d === 0;
<br>        })
<br>        .remove();
<br>      var lineFunc = d3.svg
<br>        .line()
<br>        .x(function(d) {
<br>          return xRange(d.x);
<br>        })
<br>        .y(function(d) {
<br>          return yRange(d.y);
<br>        })
<br>        .interpolate("linear");
<br>      vis
<br>        .selectAll(".tick")
<br>        .filter(function(d) {
<br>          return d === 0;
<br>        })
<br>        .remove();
<br>      var asymptotes = d3.svg
<br>        .line()
<br>        .x([-3, 3])
<br>        .y([5, 5])
<br>        .interpolate("linear");
<br>      var asymptote_top = 5,
<br>        asymptote_bottom = -5;
<br>      //d3.select("#func").remove();
<br>      vis
<br>        .append("svg:path")
<br>        .attr("d", lineFunc(lineData))
<br>        .attr("stroke", "blue")
<br>        .attr("stroke-width", 3)
<br>        .attr("id", "func")
<br>        .attr("fill", "none");
<br>      vis
<br>        .append("svg:path")
<br>        .attr("d", lineFunc(lineData_deriv))
<br>        .attr("stroke", "orange")
<br>        .attr("stroke-width", 2)
<br>        .attr("id", "deriv_func")
<br>        .attr("fill", "none");
<br>      vis
<br>        .append("svg:path")
<br>        .attr(
<br>          "d",
<br>          lineFunc([{ x: axis_limits[0], y: 5 }, { x: axis_limits[1], y: 5 }])
<br>        )
<br>        .attr("stroke", "gray")
<br>        .attr("stroke-width", 2)
<br>        .style("stroke-dasharray", "3, 3")
<br>        .attr("id", "top_asymptote")
<br>        .attr("fill", "none");
<br>      vis
<br>        .append("svg:path")
<br>        .attr(
<br>          "d",
<br>          lineFunc([{ x: axis_limits[0], y: -5 }, { x: axis_limits[1], y: -5 }])
<br>        )
<br>        .attr("stroke", "gray")
<br>        .attr("stroke-width", 2)
<br>        .style("stroke-dasharray", "3, 3")
<br>        .attr("id", "bottom_asymptote")
<br>        .attr("fill", "none");
<br>      vis
<br>        .append("rect")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin)
<br>        .attr("y", MARGINS.top)
<br>        .attr("width", MARGINS.right - rect_margin * 2.0)
<br>        .attr("height", (HEIGHT - MARGINS.top) / 2 - boxmargin)
<br>        .attr("rx", 10)
<br>        .attr("ry", 10)
<br>        .attr("stroke", "blue")
<br>        .attr("stroke-width", "5")
<br>        .attr("stroke-opacity", "0.4")
<br>        .attr("fill-opacity", "0.1")
<br>        .style("fill", "blue")
<br>        .attr("class", "info_rect")
<br>        .attr("id", "func_rect");
<br>      vis
<br>        .append("foreignObject")
<br>        .data(func_tags)
<br>        .attr("id", "func_equation")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2)
<br>        .attr("y", MARGINS.top + boxmargin / 2)
<br>        .append("xhtml:div")
<br>        .attr("class", "func_text")
<br>        .text(function(d) {
<br>          return d.func_tex;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .data(func_tags)
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2)
<br>        .attr("y", MARGINS.top + boxmargin * 5)
<br>        .attr("id", "func_range")
<br>        .attr("class", "func_text")
<br>        .text(function(d) {
<br>          return "值域: " + d.func_range;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .data(func_tags)
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2)
<br>        .attr("y", MARGINS.top + boxmargin * 6.3)
<br>        .attr("id", "func_mono")
<br>        .attr("class", "func_text")
<br>        .text(function(d) {
<br>          return "单调性: " + d.func_mono;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .data(func_tags)
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2)
<br>        .attr("y", MARGINS.top + boxmargin * 7.6)
<br>        .attr("id", "func_contin")
<br>        .attr("class", "func_text")
<br>        .text(function(d) {
<br>          return "连续性: " + d.func_contin;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .data(func_tags)
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2)
<br>        .attr("y", MARGINS.top + boxmargin * 8.9)
<br>        .attr("id", "func_origin")
<br>        .attr("class", "func_text")
<br>        .text(function(d) {
<br>          return "Identity at Origin: " + d.func_origin;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .data(func_tags)
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2)
<br>        .attr("y", MARGINS.top + boxmargin * 10.2)
<br>        .attr("id", "func_symm")
<br>        .attr("class", "func_text")
<br>        .text(function(d) {
<br>          return "对称性: " + d.func_symm;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - boxmargin_horizontal / 1.7)
<br>        .attr("y", MARGINS.top + boxmargin * 10.2)
<br>        .attr("xlink:href", "https://dashee87.github.io/")
<br>        .attr("id", "func_ref")
<br>        .attr("class", "func_text")
<br>        .attr("fill", "red")
<br>        .html("");
<br>      // derivative rectangle
<br>      vis
<br>        .append("rect")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin)
<br>        .attr("y", MARGINS.top + HEIGHT / 2)
<br>        .attr("width", MARGINS.right - rect_margin * 2.0)
<br>        .attr("height", (HEIGHT - MARGINS.top) / 2 - boxmargin)
<br>        .attr("rx", 10)
<br>        .attr("ry", 10)
<br>        .attr("stroke", "orange")
<br>        .attr("stroke-width", "5")
<br>        .attr("stroke-opacity", "0.4")
<br>        .attr("fill-opacity", "0.1")
<br>        .style("fill", "orange")
<br>        .attr("class", "info_rect)")
<br>        .attr("id", "deriv_func_rect");
<br>      vis
<br>        .append("foreignObject")
<br>        .data(deriv_func_tags)
<br>        .attr("id", "deriv_func_equation")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin / 2)
<br>        .append("xhtml:div")
<br>        .attr("class", "deriv_func_text")
<br>        .html(function(d) {
<br>          return d.deriv_func_tex;
<br>        });
<br>      if (navigator.userAgent.toLowerCase().indexOf("firefox") > -1) {
<br>        vis
<br>          .select("#func_equation")
<br>          .attr("width", MARGINS.right - rect_margin * 2)
<br>          .attr("height", boxmargin * 3);
<br>        vis
<br>          .select("#deriv_func_equation")
<br>          .attr("width", MARGINS.right - rect_margin * 2)
<br>          .attr("height", boxmargin * 3);
<br>      }
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 5)
<br>        .data(deriv_func_tags)
<br>        .attr("id", "deriv_func_range")
<br>        .attr("class", "deriv_func_text")
<br>        .text(function(d) {
<br>          return "值域: " + d.deriv_func_range;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 6.3)
<br>        .data(deriv_func_tags)
<br>        .attr("id", "deriv_func_mono")
<br>        .attr("class", "deriv_func_text")
<br>        .style("colour", "blue")
<br>        .text(function(d) {
<br>          return "单调性: " + d.deriv_func_mono;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 7.6)
<br>        .data(deriv_func_tags)
<br>        .attr("id", "deriv_func_contin")
<br>        .attr("class", "deriv_func_text")
<br>        .text(function(d) {
<br>          return "连续性: " + d.deriv_func_contin;
<br>        });
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 8.9)
<br>        .attr("id", "deriv_func_van")
<br>        .attr("class", "deriv_func_text")
<br>        .append("svg:tspan")
<br>        .text("梯度消失: ")
<br>        .append("svg:tspan")
<br>        .style("fill", "#00e600")
<br>        .text(deriv_func_tags[0].deriv_func_van);
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - boxmargin_horizontal)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 8.9)
<br>        .attr("id", "deriv_func_exp")
<br>        .attr("class", "deriv_func_text")
<br>        .append("svg:tspan")
<br>        .text("梯度爆炸: ")
<br>        .append("svg:tspan")
<br>        .style("fill", "#00e600")
<br>        .text(deriv_func_tags[0].deriv_func_exp);
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 10.3)
<br>        .attr("id", "deriv_func_sat")
<br>        .attr("class", "deriv_func_text")
<br>        .append("svg:tspan")
<br>        .text("神经元饱和: ")
<br>        .append("svg:tspan")
<br>        .style("fill", "#00e600")
<br>        .text(deriv_func_tags[0].deriv_func_sat);
<br>      vis
<br>        .append("text")
<br>        .attr("x", WIDTH - boxmargin_horizontal)
<br>        .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 10.3)
<br>        .attr("id", "deriv_func_dead")
<br>        .attr("class", "deriv_func_text")
<br>        .append("svg:tspan")
<br>        .text("神经元死亡: ")
<br>        .append("svg:tspan")
<br>        .style("fill", "red")
<br>        .text(deriv_func_tags[0].deriv_func_dead);
<br>      vis
<br>        .append("text")
<br>        .attr("x", MARGINS.left)
<br>        .attr("y", MARGINS.top + 30)
<br>        .attr("class", "legend")
<br>        .attr("id", "func_text")
<br>        .style("fill", "steelblue")
<br>        .on("click", function() {
<br>          // Determine if current line is visible
<br>          var active = func.active ? false : true,
<br>            newOpacity = active ? 0 : 1;
<br>          // Hide or show the elements
<br>          d3.select("#func").style("opacity", newOpacity);
<br>          d3.select("#func_text").style("opacity", active ? 0.5 : 1);
<br>          d3.select("#func_rect")
<br>            .transition()
<br>            .duration(1000)
<br>            .attr(
<br>              "height",
<br>              active ? 0 : (HEIGHT - MARGINS.top) / 2 - boxmargin
<br>            );
<br>          if (active) {
<br>            d3.selectAll(".func_text")
<br>              .transition()
<br>              .delay(function(d, i) {
<br>                return 400 - delay * i;
<br>              })
<br>              .style("opacity", active ? 0 : 1);
<br>          } else {
<br>            d3.selectAll(".func_text")
<br>              .transition()
<br>              .delay(function(d, i) {
<br>                return 300 + (delay * i) / 2;
<br>              })
<br>              .style("opacity", active ? 0 : 1);
<br>          }
<br>          // Update whether or not the elements are active
<br>          func.active = active;
<br>        })
<br>        .text("f (x)");
<br>      // Add the red line title
<br>      vis
<br>        .append("text")
<br>        .attr("x", MARGINS.left)
<br>        .attr("y", MARGINS.top + 30 + 30)
<br>        .attr("class", "legend")
<br>        .attr("id", "deriv_text")
<br>        .style("fill", "orange")
<br>        .on("click", function() {
<br>          // Determine if current line is visible
<br>          var active = deriv_func.active ? false : true,
<br>            newOpacity = active ? 0 : 1;
<br>          // Hide or show the elements
<br>          d3.select("#deriv_func").style("opacity", newOpacity);
<br>          d3.select("#deriv_text").style("opacity", active ? 0.5 : 1);
<br>          d3.select("#deriv_func_rect")
<br>            .transition()
<br>            .duration(1000)
<br>            .attr(
<br>              "height",
<br>              active ? 0 : (HEIGHT - MARGINS.top) / 2 - boxmargin
<br>            );
<br>          if (active) {
<br>            d3.selectAll(".deriv_func_text")
<br>              .transition()
<br>              .delay(function(d, i) {
<br>                return 400 - delay * i;
<br>              })
<br>              .style("opacity", active ? 0 : 1);
<br>          } else {
<br>            d3.selectAll(".deriv_func_text")
<br>              .transition()
<br>              .delay(function(d, i) {
<br>                return 250 + (delay * i) / 2.5;
<br>              })
<br>              .style("opacity", active ? 0 : 1);
<br>          }
<br>          // Update whether or not the elements are active
<br>          deriv_func.active = active;
<br>        })
<br>        .text("f '(x)");
<br>      d3.select(window).on("resize", function() {
<br>        vis = d3.select("#visualisation");
<br>        WIDTH = parseInt(
<br>          document.getElementById("browser_warning").offsetWidth
<br>        );
<br>        HEIGHT = parseInt(vis.style("height"));
<br>        MARGINS = {
<br>          top: HEIGHT / 25.0,
<br>          right: WIDTH / 2.5,
<br>          bottom: HEIGHT / 25.0,
<br>          left: WIDTH / 50.0
<br>        };
<br>        boxmargin = (HEIGHT - MARGINS.top) / 24.0;
<br>        rect_margin = MARGINS.right / 50.0;
<br>        (boxmargin_horizontal = MARGINS.right / 2.2),
<br>          (xRange = d3.scale
<br>            .linear()
<br>            .range([MARGINS.left, WIDTH - MARGINS.right])
<br>            .domain(axis_limits.slice(0, 2)));
<br>        yRange = d3.scale
<br>          .linear()
<br>          .range([HEIGHT - MARGINS.top, MARGINS.bottom])
<br>          .domain(axis_limits.slice(2, 4));
<br>        xAxis = d3.svg
<br>          .axis()
<br>          .scale(xRange)
<br>          .tickSize(5)
<br>          .tickSubdivide(true);
<br>        yAxis = d3.svg
<br>          .axis()
<br>          .scale(yRange)
<br>          .tickSize(5)
<br>          .orient("left")
<br>          .tickSubdivide(true);
<br>        vis
<br>          .select("#graph_xaxis")
<br>          .attr(
<br>            "transform",
<br>            "translate(0," + (HEIGHT - MARGINS.bottom + MARGINS.top) / 2 + ")"
<br>          )
<br>          .call(xAxis);
<br>        vis
<br>          .select("#graph_yaxis")
<br>          .attr(
<br>            "transform",
<br>            "translate(" + (WIDTH - MARGINS.right + MARGINS.left) / 2 + ",0)"
<br>          )
<br>          .call(yAxis);
<br>        vis
<br>          .selectAll(".tick")
<br>          .filter(function(d) {
<br>            return d === 0;
<br>          })
<br>          .remove();
<br>        lineFunc = d3.svg
<br>          .line()
<br>          .x(function(d) {
<br>            return xRange(d.x);
<br>          })
<br>          .y(function(d) {
<br>            return yRange(d.y);
<br>          })
<br>          .interpolate("linear");
<br>        vis
<br>          .select("#top_asymptote")
<br>          .attr(
<br>            "d",
<br>            lineFunc([
<br>              { x: axis_limits[0], y: asymptote_top },
<br>              { x: axis_limits[1], y: asymptote_top }
<br>            ])
<br>          );
<br>        vis
<br>          .select("#bottom_asymptote")
<br>          .attr(
<br>            "d",
<br>            lineFunc([
<br>              { x: axis_limits[0], y: asymptote_bottom },
<br>              { x: axis_limits[1], y: asymptote_bottom }
<br>            ])
<br>          );
<br>        //d3.select("#func").remove();
<br>        vis.select("#func").attr("d", lineFunc(lineData));
<br>        vis.select("#deriv_func").attr("d", lineFunc(lineData_deriv));
<br>        vis
<br>          .select("#func_rect")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin)
<br>          .attr("y", MARGINS.top)
<br>          .attr("width", MARGINS.right - rect_margin * 2.0)
<br>          .attr("height", (HEIGHT - MARGINS.top) / 2 - boxmargin);
<br>        vis
<br>          .select("#func_equation")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + boxmargin / 2);
<br>        vis
<br>          .select("#func_range")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + boxmargin * 5);
<br>        vis
<br>          .select("#func_mono")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + boxmargin * 6.3);
<br>        vis
<br>          .select("#func_contin")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + boxmargin * 7.6);
<br>        vis
<br>          .select("#func_origin")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + boxmargin * 8.9);
<br>        vis
<br>          .select("#func_symm")
<br>          .data(func_tags)
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + boxmargin * 10.2);
<br>        vis
<br>          .select("#func_ref")
<br>          .attr("x", WIDTH - boxmargin_horizontal / 1.7)
<br>          .attr("y", MARGINS.top + boxmargin * 10.2);
<br>        // derivative rectangle
<br>        vis
<br>          .select("#deriv_func_rect")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin)
<br>          .attr("y", MARGINS.top + HEIGHT / 2)
<br>          .attr("width", MARGINS.right - rect_margin * 2.0)
<br>          .attr("height", (HEIGHT - MARGINS.top) / 2 - boxmargin);
<br>        vis
<br>          .select("#deriv_func_equation")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin / 2);
<br>        vis
<br>          .select("#deriv_func_range")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 5);
<br>        vis
<br>          .select("#deriv_func_mono")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 6.3);
<br>        vis
<br>          .select("#deriv_func_contin")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 7.6);
<br>        vis
<br>          .select("#deriv_func_van")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 8.9);
<br>        vis
<br>          .select("#deriv_func_exp")
<br>          .attr("x", WIDTH - boxmargin_horizontal)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 8.9);
<br>        vis
<br>          .select("#deriv_func_sat")
<br>          .attr("x", WIDTH - MARGINS.right + rect_margin * 2.0)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 10.3);
<br>        vis
<br>          .select("#deriv_func_dead")
<br>          .attr("x", WIDTH - boxmargin_horizontal)
<br>          .attr("y", MARGINS.top + HEIGHT / 2 + boxmargin * 10.3);
<br>      });
<br>      function InitChart() {
<br>        var myClasses = document.getElementById("plot_inputs"),
<br>          mylabels = document.getElementsByTagName("label");
<br>        for (i = 0; i < myClasses.length; i++) {
<br>          myClasses[i].style.display = "none";
<br>          mylabels[i].style.display = "none";
<br>        }
<br>        document.getElementById("rrelu_button").style.display = "none";
<br>        var activ_func = document.getElementById("select_activ").value;
<br>        lineData = [];
<br>        lineData_deriv = [];
<br>        asymptote_top = 5;
<br>        asymptote_bottom = -5;
<br>        switch (activ_func) {
<br>          case "relu":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}x & \\text{for } x\\geq0\\\\0 & \\text{for } x<0\\end{cases} $",
<br>                func_range: "[0, ∞)",
<br>                func_contin: "C" + "⁰",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "不对称",
<br>                func_ref: "http://dl.acm.org/citation.cfm?id=3104322.3104425",
<br>                func_primer:
<br>                  "<b>整流线性单元</b> (<b>ReLU</b>)被认为是神经网络中最常见的激活函数。它保留了阶梯函数的生物学动机（神经元仅在输入超过阈值时触发），但在正数输入处具有非零导数，这允许基于梯度的学习（尽管导数在0时未定义）。ReLU 函数被认为有生物上的解释性，比如单侧抑制、宽兴奋边界(即兴奋程度 也可以非常高)。它的计算速度也很快，因为函数及其导数都不涉及复杂的数学运算。然而，由于在非正输入时导数为0，ReLU可能会受到缓慢学习甚至死神经元的影响，其中具有负值输入的神经元由于零值梯度而无法更新其权重，使得它们对于剩下的训练阶段沉默。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\0 & \\text{for } x<0\\end{cases} $",
<br>                deriv_func_range: "{0,1}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "Yes"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: (i > 0) * i
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: i >= 0
<br>              });
<br>            }
<br>            break;
<br>          case "step":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\0 & \\text{for } x<0\\end{cases} $",
<br>                func_range: "{0, 1}",
<br>                func_contin: "C" + "⁻¹",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "不对称",
<br>                func_ref: "",
<br>                func_primer:
<br>                "这种激活函数更具理论性而非实际性, 包括<b>Step</b> (也包括, <b>Binary Step</b> 或 <b>Heaviside</b>) 模仿生物神经元的全有或全无特性。它对神经网络没有用，因为它的导数是零（除了0，它是未定义的）。这意味着基于梯度的优化方法是不可行的"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) =\\begin{cases}0 & \\text{for } x\\neq0\\\\? & \\text{for } x=0\\end{cases} $",
<br>                deriv_func_range: "{0}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "Yes"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.3, 1.3];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: i >= 0
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 0
<br>              });
<br>            }
<br>            break;
<br>          case "sign":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}1 & x>0\\\\-1 &  x<0 \\\\0 &  x=0\\end{cases} $",
<br>                func_range: "{-1,0, 1}",
<br>                func_contin: "C" + "⁻¹",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "The <b>Signum</b> (or just <b>Sign</b>) is a scaled version of the binary step activation function. It takes the value -1 and 1 for negative and postive values, respectively, and zero at the origin. Though lacking the biological motivation of the step function, the function is anti-symmetrical, which is thought to be a favourable trait for an activation function."
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) =\\begin{cases}0 & \\text{for } x\\neq0\\\\? & \\text{for } x=0\\end{cases} $",
<br>                deriv_func_range: "{0}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "Yes"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              if (i == 0) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: 0
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: 2 * (i > 0) - 1
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 0
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "identity":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x) = x\\end{align*}$",
<br>                func_range: "(-∞, ∞)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "使用 <b>Identity</b> 使用Identity激活功能，节点输出等于其输入。它非常适合基础行为是线性的任务，类似于线性回归。当存在非线性时，单独的激活函数是不够的，尽管它仍然可以用作最终输出节点上的激活函数以用于类似回归的任务。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex: "$\\begin{align*}f'(x) = 1\\end{align*}$",
<br>                deriv_func_range: "{1}",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "✅",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: i
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 1
<br>              });
<br>            }
<br>            break;
<br>          case "sigmoid":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x) = \\frac{1}{1 + e^{-x}} \\end{align*}$",
<br>                func_range: "(0, 1)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "不对称",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "<b>Logistic Sigmoid</b>（或更常见的只是Sigmoid）激活函数从逻辑回归中的角色（0到1）中众所周知，它将概率概念引入神经网络。它的导数非零且易于计算（它是其原始输出的函数）。然而，由于Tanh是反对称的并且以原点为中心。它逐渐被Tanh取代。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x) &= \\frac{e^{-x}}{(1+e^{-x})^2} \\\\&= f(x)(1-f(x)) \\end{align*}$",
<br>                deriv_func_range: "(0,0.25)",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              asymptote_top = 1;
<br>              temp_sigmoid = 1.0 / (1.0 + Math.exp(-i));
<br>              lineData.push({
<br>                x: i,
<br>                y: temp_sigmoid
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: temp_sigmoid * (1 - temp_sigmoid)
<br>              });
<br>            }
<br>            break;
<br>          case "arctan":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x) =\\tan^{-1}(x) \\end{align*}$",
<br>                func_range: "(-π/2, π/2)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "❌",
<br>                func_origin: "✅",
<br>                func_symm: "Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "视觉上和Tanh类似，更为平坦。默认值域， (-π/2, π/2)。导数很慢趋近于0，但导数计算量大于Tanh。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)=\\frac{1}{x^2+1} \\end{align*}$",
<br>                deriv_func_range: "(0,1]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.6, 1.6];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              asymptote_top = Math.PI / 2;
<br>              asymptote_bottom = -Math.PI / 2;
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.atan(i)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 1 / (Math.pow(i, 2) + 1)
<br>              });
<br>            }
<br>            break;
<br>          case "softsign":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x)=\\frac{x}{1+|x|} \\end{align*}$",
<br>                func_range: "(0, ∞)",
<br>                func_contin: "C¹",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref:
<br>                  "http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205",
<br>                func_primer:
<br>                  "<b>Softsign</b> 和Tanh类似，默认值域， (-1, 1)。导数很慢趋近于0，但导数计算量大于Tanh。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)=\\frac{1}{(1+|x|)^2} \\end{align*}$",
<br>                deriv_func_range: "(0,1]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-6, 6, -1.6, 1.6];
<br>            for (var i = -6; i <= 6; i = i + 0.002) {
<br>              asymptote_top = 1;
<br>              asymptote_bottom = -1;
<br>              lineData.push({
<br>                x: i,
<br>                y: i / (1 + Math.abs(i))
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 1 / Math.pow(1 + Math.abs(i), 2)
<br>              });
<br>            }
<br>            break;
<br>          case "softplus":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x)=\\ln(1+e^x) \\end{align*}$",
<br>                func_range: "(0, ∞)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "不对称",
<br>                func_ref:
<br>                  "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf",
<br>                func_primer:
<br>                  "ReLu的平滑替代品，定义域上连续可导；但是不关于0对称，由于导数总<1，可能会梯度消失。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)=\\frac{1}{1+e^{-x}}\\end{align*}$",
<br>                deriv_func_range: "(0,1)",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "✅",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.log(1 + Math.exp(i), 2)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 1 / (1 + Math.exp(-i))
<br>              });
<br>            }
<br>            break;
<br>          case "bentid":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x)=\\frac{\\sqrt{x^2 + 1} - 1}{2} + x \\end{align*}$",
<br>                func_range: "(-∞, ∞)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "不对称",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "A sort of compromise between Identity and ReLU activation, <b>Bent Identity</b> allows non-linear behaviours, while its non-zero derivative promotes efficient learning and overcomes the issues of dead neurons associated with ReLU. As its derivative can return values either side of 1, it can be susceptible to both exploding and vanishing gradients."
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)=\\frac{x}{2\\sqrt{x^2 + 1}} + 1\\end{align*}$",
<br>                deriv_func_range: "(0.5, 1.5)",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "✅",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "Yes",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: (Math.sqrt(Math.pow(i, 2) + 1) - 1) / 2.0 + i
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: i / (2 * Math.sqrt(Math.pow(i, 2) + 1)) + 1
<br>              });
<br>            }
<br>            break;
<br>          case "sinusoid":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x)=\\sin(x) \\end{align*}$",
<br>                func_range: "[-1, -1]",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "❌",
<br>                func_origin: "✅",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "<b>Sinusoid</b> (or simply <b>Sin</b>)， 值域[-1,1], 原点为中心，反对称，连续可导."
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex: "$\\begin{align*}f'(x)=\\cos(x)\\end{align*}$",
<br>                deriv_func_range: "[-1, -1]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.sin(i)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: Math.cos(i)
<br>              });
<br>            }
<br>            break;
<br>          case "cos":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x)=\\cos(x) \\end{align*}$",
<br>                func_range: "[-1, -1]",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "❌",
<br>                func_origin: "❌",
<br>                func_symm: "Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "<b>Cos</b> (or <b>Cosine</b>) 值域[-1,1], 轴对称，连续可导."
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex: "$\\begin{align*}f'(x)=\\-sin(x)\\end{align*}$",
<br>                deriv_func_range: "[-1, -1]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.cos(i)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: -Math.sin(i)
<br>              });
<br>            }
<br>            break;
<br>          case "sinc":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x)=\\begin{cases}1 & \\text{for } x = 0\\\\\\frac{\\sin(x)}{x} & \\text{for } x \\ne 0\\end{cases}$",
<br>                func_range: "[≈-0.2172,1]",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "❌",
<br>                func_origin: "❌",
<br>                func_symm: "Symmetrical",
<br>                func_ref:
<br>                  "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2760/1/Function-approximation-using-a-sinc-neural-network/10.1117/12.235959.short?SSO=1",
<br>                func_primer:
<br>                  "<b>Sinc</b> (全名<b>Cardinal Sine</b> )代表了傅立叶变换的矩形函数。作为一种激活函数，它可以完全可导和对称，但它很容易梯度消失。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x)=\\begin{cases}0 & \\text{for } x = 0\\\\\\frac{\\cos(x)}{x} - \\frac{\\sin(x)}{x^2} & \\text{for } x \\ne 0\\end{cases}$",
<br>                deriv_func_range: "(≈-0.4362, ≈0.4362)",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-6, 6, -1.5, 1.5];
<br>            for (var i = -6; i <= 6; i = i + 0.002) {
<br>              if (i == 0) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: Math.sin(i) / i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: Math.sin(i) / i
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: Math.sin(i) / i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: Math.cos(i) / i - Math.sin(i) / Math.pow(i, 2)
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "tanh":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x)&=\\tanh(x)\\\\&=\\frac{2}{1+e^{-2x}}-1\\end{align*}$",
<br>                func_range: "(-1, -1)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "逐渐取代logistic sigmoid函数作为分类任务的激活函数，Tanh（Hyperbolic Tan）具有神经网络中有利的各种特征。它完全可微，以零和反对称为中心。可以采用更平坦的变形（log-log，softsign，对称sigmoid等），减轻缓慢学习和/或消失梯度的现象。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)&=1-\\tanh^2(x)\\\\&=1-f(x)^2\\end{align*}$",
<br>                deriv_func_range: "(0, 1]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            asymptote_top = 1;
<br>            asymptote_bottom = -1;
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.tanh(i)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 1 - Math.pow(Math.tanh(i), 2)
<br>              });
<br>            }
<br>            break;
<br>          case "lctanh":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x)&=1.7519\\tanh(\\frac{2}{3}x)\\end{align*}$",
<br>                func_range: "(-1.7519 , 1.7519)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref: "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf",
<br>                func_primer:
<br>                  "<b>LeCun Tanh</b> (也叫<b>Scaled Tanh</b>)，Tanh的缩放版本. 有一些提升学习能力的特性: f(± 1) = ±1; 在x=1处二阶导最大;有效增益接近1. 可以查看参考文献."
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)&=1.7519*\\frac{2}{3}(1-\\tanh^2(\\frac{2}{3}x))\\\\&=1.7519*\\frac{2}{3}-\\frac{2}{3*1.7519}f(x)^2\\end{align*}$",
<br>                deriv_func_range: "(0, 1.167933]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "Yes",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -2, 2];
<br>            asymptote_top = 1.7519;
<br>            asymptote_bottom = -1.7519;
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: 1.7519 * Math.tanh((2 * i) / 3.0)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y:
<br>                  (2.0 / 3.0) *
<br>                  1.7519 *
<br>                  (1 - Math.pow(Math.tanh((2 * i) / 3.0), 2))
<br>              });
<br>            }
<br>            break;
<br>          case "leakyrelu":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}x & \\text{for } x\\geq0\\\\0.01x & \\text{for } x<0\\end{cases} $",
<br>                func_range: "(-∞, ∞)",
<br>                func_contin: "C" + "⁰",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "不对称",
<br>                func_ref:
<br>                  "https://pdfs.semanticscholar.org/367f/2c63a6f6a10b3b64b8729d601e69337ee3cc.pdf",
<br>                func_primer:
<br>                  "经典ReLU的变形，Leaky ReLU对于负值输入，有非常小的斜率。因为导数总是非零，允许基于梯度的学习（无论多么慢），减轻了死亡神经元现象。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\0.01 & \\text{for } x<0\\end{cases} $",
<br>                deriv_func_range: "{0.01, 1}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              if (i < 0) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: 0.01 * i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 0.01
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "gaussian":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x)=e^{-x^2}\\end{align*}$",
<br>                func_range: "(0, 1]",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "❌",
<br>                func_origin: "❌",
<br>                func_symm: "Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "不要与径向基函数网络（RBFN）中常用的高斯核混淆，高斯函数不太受多层感知器类型模型的欢迎。尽管一阶导数快速收敛为零，但它完全可微分且对称。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)&=-2xe^{-x^2}\\\\ &= -2x f(x)\\end{align*}$",
<br>                deriv_func_range: "(-∞, &infin)",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.exp(-Math.pow(i, 2))
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: -2 * i * Math.exp(-Math.pow(i, 2))
<br>              });
<br>            }
<br>            break;
<br>          case "absolute":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x)=|x|\\end{align*}$",
<br>                func_range: "[0, ∞)",
<br>                func_contin: "C" + "⁰",
<br>                func_mono: "❌",
<br>                func_origin: "❌",
<br>                func_symm: "Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "顾名思义，Absolute激活函数返回输入的绝对值。它的导数很容易计算，除了0以外的任何地方定义。由于导数的大小等于1，这种激活函数不会受到消失/爆炸梯度的影响。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) =\\begin{cases}-1 & \\text{for } x<0\\\\1 & \\text{for } x>0\\\\? & \\text{for } x=0\\end{cases}$",
<br>                deriv_func_range: "{-1, 1}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -2, 2];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.abs(i)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: 2 * (i >= 0) - 1
<br>              });
<br>            }
<br>            break;
<br>          case "probit":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x)&=\\Phi(x)\\\\&=\\sqrt{2}\\,\\operatorname{erf}^{-1}(2x-1)\\end{align*}$",
<br>                func_range: "(-∞, ∞)",
<br>                func_contin: "C∞",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "❌",
<br>                func_ref: "",
<br>                func_primer: ""
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f(x)&=\\sqrt{2\\pi}\\exp^{[\\operatorname{erf}^{-1}(2x-1)]^2}\\\\&=\\sqrt{2}\\,\\operatorname{erf}^{-1}(2x-1)\\end{align*}$",
<br>                deriv_func_range: "(0, 1]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "❌",
<br>                deriv_func_sat: "❌",
<br>                deriv_func_dead: "❌"
<br>              }
<br>            ];
<br>            axis_limits = [-1, 1, -3, 3];
<br>            alert(Math.exp(-Math.pow(1, 2) / 2) / Math.sqrt(2 * Math.PI));
<br>            for (var i = 0.0005; i <= 1.0005; i = i + 0.0005 / 3) {
<br>              temp_val = Math.sqrt(2) * erfINV(2 * i - 1);
<br>              lineData.push({
<br>                x: i,
<br>                y: temp_val
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: Math.exp(-Math.pow(temp_val, 2) / 2) / Math.sqrt(2 * Math.PI)
<br>              });
<br>            }
<br>            break;
<br>          case "hardsigmoid":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}0 & \\text{for } x<-2.5\\\\0.2x + 0.5 & \\text{for } -2.5\\geq x\\leq 2.5 \\\\1 & \\text{for } x>2.5\\end{cases} $",
<br>                func_range: "[0,1]",
<br>                func_contin: "C" + "⁰",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "不对称",
<br>                func_ref: "https://arxiv.org/pdf/1603.00391.pdf",
<br>                func_primer:
<br>                  "<b>Hard Sigmoid</b>是Logistic Sigmoid激活函数的分段线性近似。它更容易计算，这使得学习速度更快。也存在零值一阶导数， 导致死神经元/慢学习（参见ReLU）。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f(x) =\\begin{cases}0 & \\text{for } x<-2.5\\\\0.2 & \\text{for } -2.5\\geq x\\leq 2.5 \\\\0 & \\text{for } x>2.5\\end{cases} $",
<br>                deriv_func_range: "{0, 0.2}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "Yes"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              if ((i >= -2.5) & (i <= 2.5)) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: 0.2 * i + 0.5
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 0.2
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: i > 2.5
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 0
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "hardtanh":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}-1 & \\text{for } x<-1\\\\x & \\text{for } -1\\geq x\\leq 1 \\\\1 & \\text{for } x>1\\end{cases} $",
<br>                func_range: "[-1,1]",
<br>                func_contin: "C" + "⁰",
<br>                func_mono: "✅",
<br>                func_origin: "✅",
<br>                func_symm: "Anti-Symmetrical",
<br>                func_ref: "https://arxiv.org/pdf/1603.00391.pdf",
<br>                func_primer:
<br>                  "<b>HardTanh</b> 是Tanh激活函数的分段线性近似。它更容易计算，这使得学习速度更快。也存在零值一阶导数， 导致死神经元/慢学习（参见ReLU）。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f(x) =\\begin{cases}0 & \\text{for } x<-1\\\\1 & \\text{for } -1\\geq x\\leq 1 \\\\0 & \\text{for } x>1\\end{cases} $",
<br>                deriv_func_range: "{0,1}",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "No",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "Yes"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.max(-1, Math.min(1, i))
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: i >= -1 && i <= 1
<br>              });
<br>            }
<br>            break;
<br>          case "symmsigmoid":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$\\begin{align*}f(x)&=\\tanh(x/2)\\\\&=\\frac{1-e^{-x}}{1+e^{-x}}\\end{align*}$",
<br>                func_range: "(-1, -1)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "Symmetrical",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "对称Sigmoid是Tanh激活的另一种替代方法（它实际上相当于Tanh激活，输入减半）。像Tanh一样，它是反对称的，零中心，可微分并且值域在[-1, 1]。它更扁平的形状和更缓慢下降的导数表明它可以更有效地学习。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f(x)&=0.5(1-\\tanh^2(x/2))\\\\&=0.5(1-f(x)^2)\\end{align*}$",
<br>                deriv_func_range: "(0, 0.5]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "✅",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            asymptote_top = 1;
<br>            asymptote_bottom = -1;
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              lineData.push({
<br>                x: i,
<br>                y: Math.tanh(i / 2)
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: (1 - Math.pow(Math.tanh(i / 2), 2)) / 2
<br>              });
<br>            }
<br>            break;
<br>          case "loglog":
<br>            func_tags = [
<br>              {
<br>                func_tex: "$\\begin{align*}f(x)=1-e^{-e^{x}}\\end{align*}$",
<br>                func_range: "(0, 1)",
<br>                func_contin: "C" + "∞",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "不对称",
<br>                func_ref: "",
<br>                func_primer:
<br>                  "函数值域(0, 1), <b>Complementary Log Log</b> (or just <b>Log Log</b>) 是sigmoid的很好的替代品. 更快饱和且在原点函数值超过0.5"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$\\begin{align*}f'(x)=e^{x}(e^{-e^{x}}) = e^{x-e^{x}}\\end{align*}$",
<br>                deriv_func_range: "(0, 1/e]",
<br>                deriv_func_contin: "✅",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -1.5, 1.5];
<br>            asymptote_top = 1;
<br>            for (var i = -3; i <= 3; i = i + 0.001) {
<br>              temp_val = Math.exp(-Math.exp(i));
<br>              lineData.push({
<br>                x: i,
<br>                y: 1 - temp_val
<br>              });
<br>              lineData_deriv.push({
<br>                x: i,
<br>                y: Math.exp(i) * temp_val
<br>              });
<br>            }
<br>            break;
<br>          case "prelu":
<br>            document.getElementById("alpha_input_label").innerHTML = "α";
<br>            document.getElementById("alpha_input_label").style.display =
<br>              "block";
<br>            document.getElementById("alpha_input").style.display = "block";
<br>            var alpha = parseFloat(
<br>              document.getElementById("alpha_input").value
<br>            );
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f_{\\alpha}(x) =\\begin{cases}x & \\text{for } x\\geq0\\\\\\alpha x & \\text{for } x<0\\end{cases} $",
<br>                func_range: "(-∞, ∞) α <0, [0, ∞) α ≥0",
<br>                func_contin: "C" + "⁰" + " (iff α ≠ 1)",
<br>                func_mono: "✅ (iff α ≥0)",
<br>                func_origin: "❌ iff α≠1",
<br>                func_symm: "asymetrical (iff |α|≠1)",
<br>                func_ref: "https://arxiv.org/abs/1502.01852",
<br>                func_primer:
<br>                  "<b>Parameteric Rectified Linear Unit</b> (<b>PReLU</b>)参数整流线性单元（PReLU），它与RReLU和Leaky ReLU有一些相似之处，因为它包含负值输入的线性项。关键的区别在于，该线的斜率实际上是作为参数在模型训练期间学习的。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'_{\\alpha}(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\\\alpha & \\text{for } x<0\\end{cases} $",
<br>                deriv_func_range: "{α, 1}",
<br>                deriv_func_contin: "❌ (iff α ≠ 1)",
<br>                deriv_func_mono: "❌ (iff α ≠ 1)",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes (iff α<1)",
<br>                deriv_func_exp: "No (iff α<1)",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No (iff α≠0)"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              if (i < 0) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: alpha * i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: alpha
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: i
<br>                });
<br>                0;
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "rrelu":
<br>            document.getElementById("rrelu_button").style.display = "block";
<br>            var alpha = 6 * Math.random() - 3;
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) =\\begin{cases}x & \\text{for } x\\geq0\\\\\\alpha x & \\text{for } x<0\\end{cases} $",
<br>                func_range: "(-∞, ∞) α <0, [0, ∞) α ≥0",
<br>                func_contin: "C" + "⁰" + " (iff α ≠ 1)",
<br>                func_mono: "✅ (iff α ≥0)",
<br>                func_origin: "❌ iff α≠1",
<br>                func_symm: "Asymetrical (iff |α|≠1)",
<br>                func_ref: "https://arxiv.org/abs/1505.00853",
<br>                func_primer:
<br>                  "<b>Randomized Leaky Rectified Linear Unit</b> (<b>RReLU</b>), 关键的区别在于该线的斜率是为每个节点随机分配的（通常来自均匀分布）"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\\\alpha & \\text{for } x<0\\end{cases} $",
<br>                deriv_func_range: "{α, 1}",
<br>                deriv_func_contin: "❌ (iff α ≠ 1)",
<br>                deriv_func_mono: "❌ (iff α ≠ 1)",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes (iff α<1)",
<br>                deriv_func_exp: "No (iff α<1)",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No (iff α≠0)"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              if (i < 0) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: alpha * i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: alpha
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "elu":
<br>            document.getElementById("alpha_input_label").innerHTML = "α";
<br>            document.getElementById("alpha_input_label").style.display =
<br>              "block";
<br>            document.getElementById("alpha_input").style.display = "block";
<br>            var alpha = parseFloat(
<br>              document.getElementById("alpha_input").value
<br>            );
<br>            asymptote_bottom = -alpha;
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) = \\begin{cases}\\alpha(e^x - 1) & \\text{for } x < 0\\\\x & \\text{for } x \\ge 0\\end{cases} $",
<br>                func_range: "(-α, ∞) α <0, [0, ∞) α ≥0",
<br>                func_contin: "C" + "⁰" + " (iff α ≠ 1)",
<br>                func_mono: "✅ (iff α ≥0)",
<br>                func_origin: "❌ iff α≠1",
<br>                func_symm: "不对称",
<br>                func_ref: "https://arxiv.org/abs/1511.07289",
<br>                func_primer:
<br>                  "<b>Exponential Linear Unit</b> (<b>ELU</b>) 包含负指数项，防止神经元死亡。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) = \\begin{cases} \\alpha e^{x} = f(x) + \\alpha & \\text{for } x < 0\\\\1 & \\text{for } x \\ge 0\\end{cases}$",
<br>                deriv_func_range: "{(0, α] ∪ 1}",
<br>                deriv_func_contin: "❌ (unless α = 1)",
<br>                deriv_func_mono: "✅ (unless |α|>1)",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "No (iff α<1)",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No (iff α≠0)"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              if (i < 0) {
<br>                temp_val = alpha * (Math.exp(i) - 1);
<br>                lineData.push({
<br>                  x: i,
<br>                  y: temp_val
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: temp_val + alpha
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: i
<br>                });
<br>                0;
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "selu":
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f(x) = \\lambda \\begin{cases}\\alpha(e^x - 1) & \\text{for } x < 0\\\\x & \\text{for } x \\ge 0 \\end{cases}\\\\\\text{ with } \\lambda = 1.0507, \\alpha = 1.67326 $",
<br>                func_range: "(-λα, ∞)",
<br>                func_contin: "C" + "⁰",
<br>                func_mono: "✅",
<br>                func_origin: "❌",
<br>                func_symm: "不对称",
<br>                func_ref: "https://arxiv.org/abs/1706.02515",
<br>                func_primer:
<br>                  "<b>Scaled Exponential Linear Unit</b> (<b>SELU</b>) ，ELU的变形, 但固定 λ = 1.0507,  α=1.67326. 这些值使得输出信号均值为0，方差为1，是自归一化神经网络的基础(Self-Normalising Neural Networks, SNNs)."
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'(x) = \\begin{cases}\\lambda\\alpha e^{x} = \\lambda(f(x) + \\alpha) & \\text{for } x < 0\\\\\\lambda & \\text{for } x \\ge 0\\end{cases}$",
<br>                deriv_func_range: "(0, λα)",
<br>                deriv_func_contin: "❌",
<br>                deriv_func_mono: "❌",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes",
<br>                deriv_func_exp: "Yes",
<br>                deriv_func_sat: "Yes",
<br>                deriv_func_dead: "No"
<br>              }
<br>            ];
<br>            asymptote_bottom = -1.0507 * 1.67326;
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              if (i < 0) {
<br>                temp_val = 1.0507 * 1.67326 * (Math.exp(i) - 1);
<br>                lineData.push({
<br>                  x: i,
<br>                  y: temp_val
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1.0507 * (temp_val + 1.67326)
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: 1.0507 * i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1.0507
<br>                });
<br>              }
<br>            }
<br>            break;
<br>          case "srelu":
<br>            document.getElementById("alpha_input").style.display = "block";
<br>            document.getElementById("alpha_input_label").style.display =
<br>              "block";
<br>            document.getElementById("alpha_input_r").style.display = "block";
<br>            document.getElementById("alpha_input_r_label").style.display =
<br>              "block";
<br>            document.getElementById("t_input_l").style.display = "block";
<br>            document.getElementById("t_input_l_label").style.display = "block";
<br>            document.getElementById("t_input_r").style.display = "block";
<br>            document.getElementById("t_input_r_label").style.display = "block";
<br>            var alpha_l = parseFloat(
<br>              document.getElementById("alpha_input").value
<br>            );
<br>            var alpha_r = parseFloat(
<br>              document.getElementById("alpha_input_r").value
<br>            );
<br>            var t_l = parseFloat(document.getElementById("t_input_l").value);
<br>            var t_r = parseFloat(document.getElementById("t_input_r").value);
<br>            document.getElementById("alpha_input_label").innerHTML =
<br>              "α<sub>l</sub>";
<br>            func_tags = [
<br>              {
<br>                func_tex:
<br>                  "$f_{t_l,a_l,t_r,a_r}(x) = \\begin{cases}t_l + a_l (x - t_l) & \\text{for } x \\le t_l\\\\x & \\text{for } t_l < x < t_r\\\\t_r + a_r (x - t_r) & \\text{for } x \\ge t_r\\end{cases}$",
<br>                func_range: "(-∞, ∞)",
<br>                func_contin: "C" + "⁰" + " (unless α_l=a_r=1)",
<br>                func_mono: "✅ (unless ∃ α<0)",
<br>                func_origin: "✅ (unless t_l>0 or t_r<0)",
<br>                func_symm: "Asymmetrical (generally)",
<br>                func_ref: "https://arxiv.org/abs/1512.07030",
<br>                func_primer:
<br>                  "<b>S-shaped Rectified Linear Activation Unit</b> (<b>SReLU</b>) ，由3段线性函数组成. 这些函数及其导数有相同的交点，在学习过程中学习这些参数。"
<br>              }
<br>            ];
<br>            deriv_func_tags = [
<br>              {
<br>                deriv_func_tex:
<br>                  "$f'_{t_l,a_l,t_r,a_r}(x) = \\begin{cases}a_l & \\text{for } x \\le t_l\\\\1   & \\text{for } t_l < x < t_r\\\\a_r & \\text{for } x \\ge t_r\\end{cases}$",
<br>                deriv_func_range: "{α_l, α_r, 1}",
<br>                deriv_func_contin: "❌ (unless α_l=α_r=1)",
<br>                deriv_func_mono: "❌ (unless α_l≤1 & α_r≥1)",
<br>                deriv_func_symm: "❌",
<br>                deriv_func_van: "Yes (iff ∃ α<1)",
<br>                deriv_func_exp: "Yes (iff ∃ α>1)",
<br>                deriv_func_sat: "No",
<br>                deriv_func_dead: "No (iff ∄ α=0)"
<br>              }
<br>            ];
<br>            axis_limits = [-3, 3, -3, 3];
<br>            for (var i = -3; i < 3; i = i + 0.001) {
<br>              if (i <= t_l) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: t_l + alpha_l * (i - t_l)
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: alpha_l
<br>                });
<br>              } else if (i >= t_r) {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: t_r + alpha_r * (i - t_r)
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: alpha_r
<br>                });
<br>              } else {
<br>                lineData.push({
<br>                  x: i,
<br>                  y: i
<br>                });
<br>                lineData_deriv.push({
<br>                  x: i,
<br>                  y: 1
<br>                });
<br>              }
<br>            }
<br>            break;
<br>        }
<br>        xRange.domain(axis_limits.slice(0, 2));
<br>        yRange.domain(axis_limits.slice(2, 4));
<br>        var vis = d3.select("#visualisation");
<br>        var lineFunc = d3.svg
<br>          .line()
<br>          .x(function(d) {
<br>            return xRange(d.x);
<br>          })
<br>          .y(function(d) {
<br>            return yRange(d.y);
<br>          })
<br>          .interpolate("linear");
<br>        var axis_ratio =
<br>          axis_limits[1] -
<br>          axis_limits[0] -
<br>          (axis_limits[3] - axis_limits[2]) / (axis_limits[1] - axis_limits[0]);
<br>        vis
<br>          .select("#func") // change the line
<br>          .transition()
<br>          .duration(1000)
<br>          .attr("d", lineFunc(lineData));
<br>        vis
<br>          .select("#deriv_func") // change the line
<br>          .transition()
<br>          .duration(1000)
<br>          .attr("d", lineFunc(lineData_deriv));
<br>        vis
<br>          .select(".y.axis") // change the y axis
<br>          .transition()
<br>          .duration(1000)
<br>          .call(yAxis);
<br>        vis
<br>          .select(".x.axis") // change the x axis
<br>          .transition()
<br>          .duration(1000)
<br>          .call(xAxis);
<br>        MathJax.Hub.Config({
<br>          tex2jax: {
<br>            inlineMath: [["$", "$"], ["\\(", "\\)"]],
<br>            processEscapes: true
<br>          }
<br>        });
<br>        MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
<br>        vis
<br>          .select("#func_equation")
<br>          .select("div")
<br>          .transition()
<br>          .text(func_tags[0].func_tex);
<br>        vis
<br>          .select("#deriv_func_equation")
<br>          .select("div")
<br>          .text(deriv_func_tags[0].deriv_func_tex);
<br>        MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
<br>        vis
<br>          .select("#func_contin")
<br>          .transition()
<br>          .text("连续性: " + func_tags[0].func_contin);
<br>        vis
<br>          .select("#func_symm")
<br>          .transition()
<br>          .text("对称性: " + func_tags[0].func_symm);
<br>        vis
<br>          .select("#func_range")
<br>          .transition()
<br>          .text("值域: " + func_tags[0].func_range);
<br>        vis
<br>          .select("#func_mono")
<br>          .transition()
<br>          .text("单调性: " + func_tags[0].func_mono);
<br>        vis
<br>          .select("#func_origin")
<br>          .transition()
<br>          .text("Identity at Origin: " + func_tags[0].func_origin);
<br>        if (func_tags[0].func_ref == "") {
<br>          vis.select("#func_ref").html("");
<br>        } else {
<br>          vis
<br>            .select("#func_ref")
<br>            .html(
<br>              '<a href="' +
<br>                func_tags[0].func_ref +
<br>                '" target="_blank" style="fill:blue">Reference</a>'
<br>            );
<br>        }
<br>        // update derivative info table
<br>        vis
<br>          .select("#deriv_func_contin")
<br>          .transition()
<br>          .text("连续性: " + deriv_func_tags[0].deriv_func_contin);
<br>        vis
<br>          .select("#deriv_func_range")
<br>          .transition()
<br>          .text("值域: " + deriv_func_tags[0].deriv_func_range);
<br>        vis
<br>          .select("#deriv_func_mono")
<br>          .transition()
<br>          .text("单调性: " + deriv_func_tags[0].deriv_func_mono);
<br>        vis
<br>          .select("#deriv_func_origin")
<br>          .transition()
<br>          .text("Identity at Origin: " + deriv_func_tags[0].deriv_func_origin);
<br>        vis
<br>          .select("#deriv_func_symm")
<br>          .transition()
<br>          .text("Identity at Origin: " + deriv_func_tags[0].deriv_func_symm);
<br>        // update asymptote lines
<br>        vis
<br>          .select("#top_asymptote")
<br>          .transition()
<br>          .duration(1000)
<br>          .attr(
<br>            "d",
<br>            lineFunc([
<br>              { x: axis_limits[0], y: asymptote_top },
<br>              { x: axis_limits[1], y: asymptote_top }
<br>            ])
<br>          );
<br>        vis
<br>          .select("#bottom_asymptote")
<br>          .transition()
<br>          .duration(1000)
<br>          .attr(
<br>            "d",
<br>            lineFunc([
<br>              { x: axis_limits[0], y: asymptote_bottom },
<br>              { x: axis_limits[1], y: asymptote_bottom }
<br>            ])
<br>          );
<br>        vis
<br>          .selectAll(".tick")
<br>          .filter(function(d) {
<br>            return d === 0;
<br>          })
<br>          .remove();
<br>        document.getElementById("func_primer").innerHTML =
<br>          func_tags[0].func_primer;
<br>        if (
<br>          activ_func == "srelu" ||
<br>          activ_func == "prelu" ||
<br>          activ_func == "rrelu" ||
<br>          activ_func == "elu"
<br>        ) {
<br>          vis
<br>            .select("#deriv_func_exp")
<br>            .select("tspan")
<br>            .text("Exploding: ")
<br>            .append("svg:tspan")
<br>            .style(
<br>              "fill",
<br>              deriv_func_tags[0].deriv_func_exp.indexOf("Yes") > -1
<br>                ? "red"
<br>                : "#00e600"
<br>            )
<br>            .text(deriv_func_tags[0].deriv_func_exp);
<br>          vis
<br>            .select("#deriv_func_van")
<br>            .select("tspan")
<br>            .text("Vanishing: ")
<br>            .append("svg:tspan")
<br>            .style(
<br>              "fill",
<br>              deriv_func_tags[0].deriv_func_van.indexOf("Yes") > -1
<br>                ? "red"
<br>                : "#00e600"
<br>            )
<br>            .text(deriv_func_tags[0].deriv_func_van);
<br>        } else {
<br>          vis
<br>            .select("#deriv_func_exp")
<br>            .select("tspan")
<br>            .text("梯度爆炸: ")
<br>            .append("svg:tspan")
<br>            .style(
<br>              "fill",
<br>              deriv_func_tags[0].deriv_func_exp.indexOf("Yes") > -1
<br>                ? "red"
<br>                : "#00e600"
<br>            )
<br>            .text(deriv_func_tags[0].deriv_func_exp);
<br>          vis
<br>            .select("#deriv_func_van")
<br>            .select("tspan")
<br>            .text("梯度消失: ")
<br>            .append("svg:tspan")
<br>            .style(
<br>              "fill",
<br>              deriv_func_tags[0].deriv_func_van.indexOf("Yes") > -1
<br>                ? "red"
<br>                : "#00e600"
<br>            )
<br>            .text(deriv_func_tags[0].deriv_func_van);
<br>        }
<br>        vis
<br>          .select("#deriv_func_sat")
<br>          .select("tspan")
<br>          .select("tspan")
<br>          .style(
<br>            "fill",
<br>            deriv_func_tags[0].deriv_func_sat.indexOf("Yes") > -1
<br>              ? "red"
<br>              : "#00e600"
<br>          )
<br>          .text(deriv_func_tags[0].deriv_func_sat);
<br>        vis
<br>          .select("#deriv_func_dead")
<br>          .select("tspan")
<br>          .select("tspan")
<br>          .style(
<br>            "fill",
<br>            deriv_func_tags[0].deriv_func_dead.indexOf("Yes") > -1
<br>              ? "red"
<br>              : "#00e600"
<br>          )
<br>          .text(deriv_func_tags[0].deriv_func_dead);
<br>      }
<br>    </script>
<br>  </body>
<br></html>
<br>{% endraw %}
<br>